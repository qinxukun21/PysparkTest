{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c8ad817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c04f214",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   读入数据\n",
    "true_news = pd.read_csv('./data/true.csv')\n",
    "fake_news = pd.read_csv('./data/fake.csv')\n",
    "#   打上标签\n",
    "true_news[\"lable\"] = [1] * 21417\n",
    "fake_news[\"lable\"] = [0] * 23481\n",
    "#   合并数据并且分割x，y\n",
    "df_all = pd.concat([true_news, fake_news]).reset_index()\n",
    "df_no = df_all.copy()\n",
    "#   观察数据，发现fake_news里边有脏，纯链接形式的，日期时间开头为\"https://\"，因此筛一下；此外，还有一条title为\"Homepage\"，应该是爬数据的时候首页错误，因此也筛去。提前用jupyter得到了下标\n",
    "for i in [30775, 36924, 36925, 37256, 37257, 38849, 38850, 40350, 43286, 43287]:\n",
    "    df_all = df_all.drop(index=i)\n",
    "df_all = df_all.drop('index',axis=1)\n",
    "df_all = df_all.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "869d2638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PorterStemmer词干提取与还原、去停用词\n",
    "title = []\n",
    "for i in df_all[\"title\"]:\n",
    "    tokens = nltk.word_tokenize(i)\n",
    "    porter = nltk.PorterStemmer()\n",
    "    title.append(\" \".join([porter.stem(t) for t in tokens]))\n",
    "text = []\n",
    "for i in df_all[\"text\"]:\n",
    "    tokens = nltk.word_tokenize(i)\n",
    "    porter = nltk.PorterStemmer()\n",
    "    text.append(\" \".join([porter.stem(t) for t in tokens]))\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "for w in ['!',',','.','?','-s','-ly','</s>','s',':','，','@']:\n",
    "    stop_words.append(w)\n",
    "title_ok = []\n",
    "for i in title:\n",
    "    tokens = nltk.word_tokenize(i)\n",
    "    title_ok.append(\" \".join([x for x in tokens if x not in stop_words]))\n",
    "text_ok = []\n",
    "for i in text:\n",
    "    tokens = nltk.word_tokenize(i)\n",
    "    text_ok.append(\" \".join([x for x in tokens if x not in stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5576838",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[\"title\"] = title_ok\n",
    "df_all[\"text\"] = text_ok\n",
    "df_all = df_all.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15c6b704",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv('./data/train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "170255b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44888"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6744fdec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23471"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_all[df_all['lable']==0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
